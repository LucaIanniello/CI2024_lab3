{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Copyright **`(c)`** 2024 Giovanni Squillero `<giovanni.squillero@polito.it>`  \n",
    "[`https://github.com/squillero/computational-intelligence`](https://github.com/squillero/computational-intelligence)  \n",
    "Free under certain conditions â€” see the [`license`](https://github.com/squillero/computational-intelligence/blob/master/LICENSE.md) for details.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from random import choice\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from icecream import ic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUZZLE_DIM = 4\n",
    "action = namedtuple('Action', ['pos1', 'pos2']) \n",
    "#to store a tuple sublass with named fields \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def available_actions(state: np.ndarray) -> list['Action']:\n",
    "    x, y = [int(_[0]) for _ in np.where(state == 0)] # row,column\n",
    "    actions = list()\n",
    "    if x > 0:\n",
    "        actions.append(action((x, y), (x - 1, y)))\n",
    "    if x < PUZZLE_DIM - 1:\n",
    "        actions.append(action((x, y), (x + 1, y)))\n",
    "    if y > 0:\n",
    "        actions.append(action((x, y), (x, y - 1)))\n",
    "    if y < PUZZLE_DIM - 1:\n",
    "        actions.append(action((x, y), (x, y + 1)))\n",
    "    return actions\n",
    "\n",
    "\n",
    "\n",
    "def do_action(state: np.ndarray, action: 'Action') -> np.ndarray:\n",
    "    new_state = state.copy()\n",
    "    new_state[action.pos1], new_state[action.pos2] = new_state[action.pos2], new_state[action.pos1]\n",
    "    return new_state\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| goal: array([[ 1,  2,  3,  4],\n",
      "                 [ 5,  6,  7,  8],\n",
      "                 [ 9, 10, 11, 12],\n",
      "                 [13, 14, 15,  0]])\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "\n",
    "\n",
    "def counter(fn):\n",
    "    \"\"\"Simple decorator for counting number of calls\"\"\"\n",
    "\n",
    "    @functools.wraps(fn)\n",
    "    def helper(*args, **kargs):\n",
    "        helper.calls += 1\n",
    "        return fn(*args, **kargs)\n",
    "\n",
    "    helper.calls = 0\n",
    "    return helper\n",
    "\n",
    "\n",
    "goal = np.array([i for i in range(1, PUZZLE_DIM**2)] + [0]).reshape((PUZZLE_DIM, PUZZLE_DIM))\n",
    "ic(goal)\n",
    "\n",
    "def goal_test(state: np.ndarray):\n",
    "    return np.array_equal(state, goal)\n",
    "\n",
    "@counter\n",
    "def difference_from_goal(state: np.ndarray) -> int:\n",
    "    return np.sum(state != np.array([i for i in range(1, PUZZLE_DIM**2)] + [0]).reshape((PUZZLE_DIM, PUZZLE_DIM)))\n",
    "    \n",
    "\n",
    "def manhattan_distance(state: np.ndarray) -> int:\n",
    "    \"\"\"Calculate the Manhattan distance of the current state to the goal state.\"\"\"\n",
    "    distance = 0\n",
    "    goal_positions = {i: (i // PUZZLE_DIM, i % PUZZLE_DIM) for i in range(1, PUZZLE_DIM ** 2)}\n",
    "    for x in range(PUZZLE_DIM):\n",
    "        for y in range(PUZZLE_DIM):\n",
    "            value = state[x, y]\n",
    "            if value != 0:\n",
    "                goal_x, goal_y = goal_positions[value]\n",
    "                distance += abs(x - goal_x) + abs(y - goal_y)\n",
    "    return distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6873180c8695469098347d738f471f73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Randomizing:   0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 4, 14, 11, 10],\n",
       "       [ 6,  2,  3,  1],\n",
       "       [ 8,  5, 12,  9],\n",
       "       [ 7,  0, 15, 13]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RANDOMIZE_STEPS = 100_000\n",
    "state = np.array([i for i in range(1, PUZZLE_DIM**2)] + [0]).reshape((PUZZLE_DIM, PUZZLE_DIM))\n",
    "for r in tqdm(range(RANDOMIZE_STEPS), desc='Randomizing'):\n",
    "    state = do_action(state, choice(available_actions(state)))\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State: \n",
      "[[ 4 14 11 10]\n",
      " [ 6  2  3  1]\n",
      " [ 8  5 12  9]\n",
      " [ 7  0 15 13]]\n",
      "Number of actions evaluated: \n",
      "440497\n",
      "Final State:\n",
      "[[ 1  2  3  4]\n",
      " [ 5  6  7  8]\n",
      " [ 9 10 11 12]\n",
      " [13 14 15  0]]\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "\n",
    "BEAM_SIZE = 4\n",
    "\n",
    "priority_queue = []\n",
    "visited = set()\n",
    "\n",
    "\n",
    "# Add the initial state to the queue\n",
    "initial_priority = difference_from_goal(state)\n",
    "heapq.heappush(priority_queue, (initial_priority, tuple(map(tuple, state))))  # Convert initial state to tuple format nparray gives problems\n",
    "\n",
    "print(\"Initial State: \")\n",
    "print(state)\n",
    "\n",
    "while priority_queue and not goal_test(state):\n",
    "    \n",
    "\n",
    "    # Pop the state with the highest priority (lowest difference from goal)\n",
    "    priority, state_tuple = heapq.heappop(priority_queue)\n",
    "    state = np.array(state_tuple)\n",
    "\n",
    "    # Add to visited to avoid re-processing this state\n",
    "    visited.add(state_tuple)\n",
    "\n",
    "    # Generate new states from the current state\n",
    "    for _ in range(BEAM_SIZE):\n",
    "        new_state = do_action(state, choice(available_actions(state))) #randomly select one possible action between legal ones\n",
    "        new_state_tuple = tuple(map(tuple, new_state)) # heapq doesn't work with nparrays \n",
    "\n",
    "        # Only add the new state if it hasn't been visited\n",
    "        if new_state_tuple not in visited:\n",
    "            heapq.heappush(priority_queue, (0.8 * difference_from_goal(new_state) + 0.2 * manhattan_distance(new_state), new_state_tuple))\n",
    "\n",
    "# Final output\n",
    "print(\"Number of actions evaluated: \")\n",
    "print(difference_from_goal.calls)\n",
    "\n",
    "print(\"Final State:\")\n",
    "print(state)\n",
    "\n",
    "#Suggested parameters for priority estimation:\n",
    "#N = 3, 4 only difference_from_goal is sufficient\n",
    "#N = 5 use 80% difference and 20% Manhattan distance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
